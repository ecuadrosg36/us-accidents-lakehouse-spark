# US Accidents Spark Lakehouse ğŸš€

A production-ready **Apache Spark Lakehouse** platform for processing and analyzing US traffic accident data using the **Medallion Architecture** (Bronze â†’ Silver â†’ Gold).

![Python](https://img.shields.io/badge/Python-3.10%2B-blue)
![Spark](https://img.shields.io/badge/Spark-3.5%2B-orange)
![Airflow](https://img.shields.io/badge/Airflow-2.7%2B-green)
![Docker](https://img.shields.io/badge/Docker-Ready-blue)

## ğŸŒŸ Features

- **Medallion Architecture**: Bronze (raw) â†’ Silver (cleaned) â†’ Gold (ML-ready)
- **Spark Optimization**: AQE, broadcast joins, dynamic partitioning
- **ML Pipeline**: RandomForest model for severity prediction
- **Streaming**: Structured Streaming with checkpointing & watermarking
- **Orchestration**: Airflow DAG with TaskFlow API
- **Infrastructure as Code**: Terraform for GCP/Dataproc
- **CI/CD**: GitHub Actions with testing & Docker builds

## ğŸ—ï¸ Architecture

```
data/raw (CSV)
    â†“
Bronze Layer (Parquet, schema-enforced, partitioned)
    â†“
Silver Layer (cleaned, deduplicated, validated)
    â†“
Gold Layer (ML features, optimized)
    â†“
ML Model (RandomForest classifier)
```

## ğŸš€ Quick Start

### Prerequisites
- Python 3.10+
- Apache Spark 3.5+
- Docker (optional)

### Installation

```bash
# Clone repository
git clone <repo-url>
cd us-accidents-lakehouse-spark

# Install dependencies
pip install -r requirements.txt

# Copy environment file
cp .env.example .env
```

### Run the Pipeline

**1. Bronze Layer (CSV â†’ Parquet)**
```bash
python -m src.jobs.bronze_ingestion
```

**2. Silver Layer (Data Quality)**
```bash
python -m src.jobs.silver_transformation
```

**3. Gold Layer (Features)**
```bash
python -m src.jobs.gold_features
```

**4. ML Training**
```bash
python -m src.ml.train_model
```

**5. ML Inference**
```bash
python -m src.ml.inference
```

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config/           # Configuration loader
â”‚   â”œâ”€â”€ schemas/          # Data schemas
â”‚   â”œâ”€â”€ jobs/             # ETL jobs (Bronze/Silver/Gold)
â”‚   â”œâ”€â”€ ml/               # ML training & inference
â”‚   â””â”€â”€ utils/            # Spark session, logging, optimization
â”œâ”€â”€ orchestration/
â”‚   â””â”€â”€ airflow/dags/     # Airflow DAGs
â”œâ”€â”€ infra/
â”‚   â””â”€â”€ terraform/        # Infrastructure as Code
â”œâ”€â”€ tests/                # Unit tests
â”œâ”€â”€ config/               # YAML configs
â””â”€â”€ Dockerfile            # Docker image
```

## ğŸ³ Docker

```bash
# Build image
docker build -t us-accidents-spark:latest .

# Run Bronze job
docker run --rm -v $(pwd)/data:/app/data us-accidents-spark:latest
```

## â˜ï¸ Cloud Deployment (GCP)

```bash
cd infra/terraform

# Initialize
terraform init

# Plan
terraform plan -var="project_id=YOUR_PROJECT_ID"

# Apply
terraform apply -var="project_id=YOUR_PROJECT_ID"
```

## ğŸ§ª Testing

```bash
# Run all tests
pytest tests/ -v

# With coverage
pytest tests/ --cov=src --cov-report=html
```

## ğŸ“Š Data Quality

- **Silver Layer DQ Rules**:
  - Null handling (drop critical, fill weather)
  - Deduplication (by ID)
  - Range validation (Severity 1-4, Lat/Lng bounds)
  - String cleaning (trim, uppercase codes)

## ğŸ¤– ML Model

- **Target**: Accident Severity (1-4)
- **Algorithm**: RandomForest (50 trees, depth 10)
- **Features**: 
  - Temporal (hour, day_of_week, is_weekend, is_rush_hour)
  - Weather (temperature, humidity, visibility, wind)
  - Infrastructure (count of traffic elements)
  - Location (lat/lng)

## ğŸ”— Contributing

1. Fork the repository
2. Create feature branch (`git checkout -b feature/amazing`)
3. Commit changes (`git commit -m 'Add feature'`)
4. Push to branch (`git push origin feature/amazing`)
5. Open a Pull Request

## ğŸ“ License

MIT License
